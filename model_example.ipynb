{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os, requests, torch, transformers, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from io import BytesIO\n",
    "from torchvision.utils import make_grid\n",
    "from monai import transforms as monai_transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SingleCXREncoderDecoderModel were not initialized from the model checkpoint at aehrc/cxrmate-single-tf and are newly initialized: ['decoder.bert.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The class ConvNextFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ConvNextImageProcessor instead.\n"
     ]
    }
   ],
   "source": [
    "ckpt_name = 'aehrc/cxrmate-single-tf'\n",
    "\n",
    "encoder_decoder = transformers.AutoModel.from_pretrained(ckpt_name, trust_remote_code=True).to(device)\n",
    "encoder_decoder.eval()\n",
    "tokenizer = transformers.PreTrainedTokenizerFast.from_pretrained(ckpt_name)\n",
    "image_processor = transformers.AutoFeatureExtractor.from_pretrained(ckpt_name)\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=image_processor.size['shortest_edge']),\n",
    "        transforms.CenterCrop(size=[\n",
    "            image_processor.size['shortest_edge'],\n",
    "            image_processor.size['shortest_edge'],\n",
    "        ]\n",
    "        ),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=image_processor.image_mean,\n",
    "            std=image_processor.image_std,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-2.0665),\n",
       " tensor(2.1694),\n",
       " torch.Size([1, 3, 384, 384]),\n",
       " ((1, 235), (1, 235), (1, 235)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pil_image = Image.open('CXR1_1_IM-0001-3001.png')\n",
    "pil_image = pil_image.convert('RGB')\n",
    "image = test_transforms(pil_image)\n",
    "image = torch.stack([image], dim=0)\n",
    "image.min(), image.max(), image.shape, pil_image.getextrema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1, 966, 306, 120, 237,  23, 139, 356, 148, 386, 349, 150, 237,  23,\n",
       "         139, 230, 705, 120, 237,  23, 661, 150, 303,  23, 198, 183, 171, 214,\n",
       "         211, 120, 269,  23, 213, 150, 163, 271, 542, 666,  23,   3, 159, 271,\n",
       "         397, 578,  23,   2]], device='mps:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = encoder_decoder.generate(\n",
    "    pixel_values=image.to(device),\n",
    "    special_token_ids=[tokenizer.sep_token_id],\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    return_dict_in_generate=True,\n",
    "    use_cache=True,\n",
    "    max_length=256,\n",
    "    num_beams=4,\n",
    ")\n",
    "outputs.sequences\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings: Heart size is normal. The mediastinal and hilar contours are normal. The pulmonary vasculature is normal. Lungs are clear. No pleural effusion or pneumothorax is seen. There are no acute osseous abnormalities.\n",
      "Impression: No acute cardiopulmonary abnormality.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Findings and impression sections (exclude previous impression section):\n",
    "findings, impression = encoder_decoder.split_and_decode_sections(\n",
    "    outputs.sequences,\n",
    "    [tokenizer.sep_token_id, tokenizer.eos_token_id],\n",
    "    tokenizer,\n",
    ")\n",
    "for i, j in zip(findings, impression):\n",
    "    print(f'Findings: {i}\\nImpression: {j}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 384, 384])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# image, tags = monai_transforms.LoadImage(image_only=False)('CXR1_1_IM-0001-3001.png')\n",
    "\n",
    "import pydicom         \n",
    "\n",
    "dataset = pydicom.dcmread('1_IM-0001-3001.dcm')\n",
    "# dataset.file_meta.TransferSyntaxUID = pydicom.uid.ImplicitVRLittleEndian  # or whatever is the correct transfer syntax for the file\n",
    "# dataset = dcmread('e084de3b-be89b11e-20fe3f9f-9c8d8dfe-4cfd202c.dcm', force=True)\n",
    "\n",
    "image = transforms.ToTensor()(dataset.pixel_array.astype(np.float32))\n",
    "\n",
    "quantisation_error = True\n",
    "\n",
    "post_dicom_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=image_processor.size['shortest_edge'], antialias=True),\n",
    "        transforms.CenterCrop(size=[\n",
    "            image_processor.size['shortest_edge'],\n",
    "            image_processor.size['shortest_edge'],\n",
    "        ]\n",
    "        ),\n",
    "        transforms.Normalize(\n",
    "            mean=image_processor.image_mean,\n",
    "            std=image_processor.image_std,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "min_intensity, max_intensity = image.min(), image.max()\n",
    "\n",
    "# See 8.1.1 for more details: https://dicom.nema.org/medical/dicom/current/output/html/part05.html#sect_8.1.1\n",
    "if ((0x28,0x106) in dataset) and ((0x28,0x107) in dataset):\n",
    "    min_intensity, max_intensity = dataset[0x28,0x106].value, dataset[0x28,0x107].value\n",
    "\n",
    "image = (image - min_intensity) / (max_intensity - min_intensity)\n",
    "# image = image.permute([2, 0, 1])\n",
    "\n",
    "if quantisation_error:\n",
    "    image = (255*image).to(torch.uint8).to(torch.float32)/255\n",
    "\n",
    "if image.shape[0] == 1:\n",
    "    image = image.repeat([3, 1, 1])\n",
    "image = post_dicom_transforms(image)\n",
    "image = torch.stack([image], dim=0)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  668,  148,  369,  546,  132,  115,  250,  854,   23,  213,  120,\n",
       "          163,  322,  284,   21,  171,   21,  214,  211,   23,  139,  449,  278,\n",
       "          120,  237,   23, 1427,  542,  569,  150,  721,   23,  198,  801,  541,\n",
       "          780,  115,  182,  615,  120,  269,   23,    3,  159,  271,  916,  389,\n",
       "           23,    2]], device='mps:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = encoder_decoder.generate(\n",
    "    pixel_values=image.to(device),\n",
    "    special_token_ids=[tokenizer.sep_token_id],\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    return_dict_in_generate=True,\n",
    "    use_cache=True,\n",
    "    max_length=256,\n",
    "    num_beams=4,\n",
    ")\n",
    "outputs.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Findings: PA and lateral views of the chest provided. There is no focal consolidation, effusion, or pneumothorax. The cardiomediastinal silhouette is normal. Imaged osseous structures are intact. No free air below the right hemidiaphragm is seen.\n",
      "Impression: No acute intrathoracic process.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Findings and impression sections (exclude previous impression section):\n",
    "findings, impression = encoder_decoder.split_and_decode_sections(\n",
    "    outputs.sequences,\n",
    "    [tokenizer.sep_token_id, tokenizer.eos_token_id],\n",
    "    tokenizer,\n",
    ")\n",
    "for i, j in zip(findings, impression):\n",
    "    print(f'Findings: {i}\\nImpression: {j}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medisynth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
